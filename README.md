<h1> Analysis suite for light-sheet fluorescence microscopy images </h1>

<h2> Project overview </h2>

This project is a collection of tools for analyzing light-sheet fluorescence microscopy images.

<h2> Core functionalities </h2>

This suite handles image alignment, nuclear instance segmentation (NIS), and morphological feature extraction.

<h3> Image Alignment </h3>
Image alignment is performed using BigStitcher-Spark, a parallelized version of BigStitcher designed for large datasets. Documentation can be found here:


-   (https://github.com/JaneliaSciComp/BigStitcher-Spark/blob/main/README.md). 

<h3> Environment Setup </h3>

To run this pipeline, you will need to clone the repo:

`$ git clone 

**Input/Output Settings (from `config.yaml`)**

-   `input_dir`: The folder containing the raw image data (e.g., `.h5` files).
-   `input_xml`: The path to the metadata file (e.g., `.xml`) describing the raw image dataset, often generated by the microscope or pre-processing software.

**BigStitcher-Spark Settings (from `config.yaml`)**

-   `bigstitcher_script_dir`: The location of the BigStitcher-Spark scripts needed to run the alignment process.

The major steps of the alignment process are:

- Stitching
- Global optimization
- Fusion

**Spark Cluster Configuration (from `config.yaml`)**

This section details the settings for the computational cluster (Spark) used for the alignment. These parameters control how much computational power is allocated.

-   `runtime`: Maximum time allocated for the alignment job (Format: HH:MM).
-   `n_nodes`: Number of computers (nodes) to use in the cluster.
-   `spark_log_base_dir`: The directory where logs (records of the process) from Spark will be saved.
-   `spark_job_timeout`: How long (in seconds) the system should wait for Spark job logs before timing out.
-   `cluster`: Specific settings for how Spark distributes the work:
    -   `executors_per_node`: Number of parallel processes (executors) to run on each computer node.
    -   `cores_per_executor`: Number of processor cores assigned to each executor.
    -   `overhead_cores_per_worker`: Extra cores reserved on each node for system tasks.
    -   `tasks_per_executor_core`: How many small tasks each core within an executor can handle concurrently.
    -   `cores_driver`: Number of cores dedicated to the main coordinating process (driver).
    -   `gb_per_slot`: Amount of memory (in Gigabytes) allocated per processing slot. *Note: This might be related to older cluster managers.*
    -   `ram_per_core`: Amount of memory (RAM) allocated for each processor core (e.g., "12G").
    -   `project`: The computational account or project code to bill the cluster usage against.
    -   `queue`: The specific queue (group of computing resources) to submit the job to on the cluster.

**Stitching (from `config.yaml`)**

This step finds the spatial relationships between adjacent image tiles.

-   `stitching_channel`: The image channel (e.g., DAPI stain) used to find overlaps between tiles. Channel numbers usually start from 0.
-   `min_correlation`: A threshold (0 to 1) indicating how similar overlapping regions must be to be considered a match. Higher values mean stricter matching.

**Global Optimization**

This step refines the positions of all tiles simultaneously based on the stitching results to create the most accurate overall alignment. (Parameters for this step are not currently detailed in the provided `config.yaml` example but might be specified elsewhere or use defaults).

**Fusion (from `config.yaml`)**

This step combines the aligned tiles into a single output image file.

-   `channels`: A list of the image channels to include in the final fused image (e.g., `[0, 1, 2]`).
-   `block_size`: The size of the data chunks (in pixels: X, Y, Z) used when writing the output file (e.g., "512,512,512"). This affects performance and compatibility with downstream tools.
-   `intensity`: Defines the range of pixel brightness values in the output.
    -   `min`: The minimum intensity value.
    -   `max`: The maximum intensity value (e.g., 65535 for 16-bit images).
-   `data_type`: The numerical format for storing pixel brightness (e.g., `UINT16` for 16-bit unsigned integers).

The fused output is typically saved in `.n5` or `.zarr` format, which are efficient for large multi-dimensional datasets.

<h3> Distributed Processing (Dask Configuration from `config.yaml`) </h3>

For tasks like segmentation and feature extraction that can be parallelized, the Dask library is used to manage computation across multiple workers (potentially using GPUs for acceleration).

-   `dask`: Container for Dask settings.
    -   `log_dir`: Directory where Dask worker logs are saved.
    -   `runtime`: Estimated runtime for Dask jobs (may influence resource allocation).
    -   **GPU Worker Settings:** Controls workers utilizing Graphics Processing Units (GPUs).
        -   `gpu_project`: Computational account for GPU usage.
        -   `gpu_queue`: Cluster queue for GPU jobs.
        -   `num_gpu_workers`: Number of GPU worker processes to start.
        -   `gpu_cores`: CPU cores allocated per GPU worker.
        -   `gpu_memory`: System memory (RAM) allocated per GPU worker (e.g., "60G").
        -   `gpu_resource_spec`: Specific hardware requirements for GPU workers (e.g., requesting 1 GPU, CUDA compatibility, minimum free GPU memory).
        -   `gpu_processes`: Number of Python processes to run within each GPU worker.
    -   **CPU Worker Settings:** Controls workers utilizing standard Central Processing Units (CPUs).
        -   `cpu_project`: Computational account for CPU usage.
        -   `cpu_queue`: Cluster queue for CPU jobs.
        -   `num_cpu_workers`: Number of CPU worker processes to start.
        -   `cpu_cores`: CPU cores allocated per CPU worker.
        -   `cpu_memory`: System memory (RAM) allocated per CPU worker (e.g., "60G").
        -   `cpu_resource_spec`: Specific hardware requirements for CPU workers (e.g., minimum free system memory).
        -   `cpu_processes`: Number of Python processes to run within each CPU worker.

<h3> Segmentation (from `config.yaml`) </h3>

This process identifies objects of interest (e.g., cells, nuclei) within the fused image.

-   `segmentation`: Container for segmentation settings.
    -   `script`: Path to the Python script that performs the segmentation (e.g., using Cellpose).
    -   `output_suffix`: Text added to the input filename to create the output segmentation filename (e.g., "_segmented_normalized.zarr").
    -   `block_size`: Processing chunk size (X, Y, Z) for the segmentation algorithm.
    -   `eval_kwargs`: Specific parameters passed directly to the segmentation model (e.g., Cellpose `eval` function), controlling thresholds, batch sizes, etc. These often require knowledge of the specific segmentation tool being used.
    -   `cellpose_model_path`: Path to the pre-trained Cellpose model used for segmentation.
    -   `n5_channel_path`: Specifies the path *within* the fused N5/Zarr file to the specific channel data that will be used as input for segmentation (e.g., "ch2/s0" means channel 2, scale 0).

<h3> Feature Extraction (from `config.yaml`) </h3>

After segmentation, this step measures various properties (features) of the segmented objects (e.g., size, shape, intensity in different channels).

-   `feature_extraction`: Container for feature extraction settings.
    -   `channels`: List of channels from which to extract features for each segmented object.
    -   `script`: Path to the Python script performing the feature extraction.
    -   `n5_path_pattern`: A template defining how to access data for different channels within the N5/Zarr file (e.g., "ch{}/s0", where {} is replaced by the channel number).
    -   `chunk_size`: The core size of the data chunks processed during feature extraction (X, Y, Z).
    -   `overlap`: The amount of overlap (in pixels: X, Y, Z) between adjacent processing chunks to ensure features are calculated correctly near chunk boundaries.

<h2> Running the Pipeline </h2>

(Information on how to execute the different steps of the analysis pipeline should be added here.)

<h2> Requirements </h2>

(List software dependencies, required libraries, and system requirements here.)

