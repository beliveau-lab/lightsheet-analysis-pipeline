Worker started on b001.grid.gs.washington.edu at Tue Jul  8 12:32:00 PDT 2025
2025-07-08 12:32:08,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.110.100.15:32851'
2025-07-08 12:32:11,510 - distributed.worker - INFO -       Start worker at:  tcp://10.110.100.15:38927
2025-07-08 12:32:11,511 - distributed.worker - INFO -          Listening to:  tcp://10.110.100.15:38927
2025-07-08 12:32:11,511 - distributed.worker - INFO -           Worker name:               SGECluster-0
2025-07-08 12:32:11,511 - distributed.worker - INFO -          dashboard at:        10.110.100.15:40075
2025-07-08 12:32:11,511 - distributed.worker - INFO - Waiting to connect to:  tcp://10.110.100.15:35189
2025-07-08 12:32:11,511 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:32:11,511 - distributed.worker - INFO -               Threads:                          2
2025-07-08 12:32:11,511 - distributed.worker - INFO -                Memory:                  27.94 GiB
2025-07-08 12:32:11,511 - distributed.worker - INFO -       Local Directory: /tmp/18930494.1.beliveau-long.q/dask-scratch-space/worker-vqfpczit
2025-07-08 12:32:11,511 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:32:15,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-07-08 12:32:15,341 - distributed.worker - INFO -         Registered to:  tcp://10.110.100.15:35189
2025-07-08 12:32:15,341 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:32:15,342 - distributed.core - INFO - Starting established connection to tcp://10.110.100.15:35189
2025-07-08 12:32:27,393 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-07-08 12:32:42,490 - INFO - Successfuly imported local module
2025-07-08 12:32:42,490 - INFO - --- Environment Versions ---
2025-07-08 12:32:42,490 - INFO - Platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35
2025-07-08 12:32:42,490 - INFO - Python: 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:03) [GCC 13.3.0]
2025-07-08 12:32:42,490 - INFO - Dask: 2025.2.0
2025-07-08 12:32:42,490 - INFO - Distributed: 2025.2.0
2025-07-08 12:32:42,490 - INFO - Cloudpickle: 3.0.0
2025-07-08 12:32:42,490 - INFO - Msgpack: 1.0.8
2025-07-08 12:32:42,490 - INFO - Zarr: 2.13.3
2025-07-08 12:32:42,490 - INFO - NumPy: 2.2.6
2025-07-08 12:32:42,490 - INFO - Scikit-image: 0.25.0
2025-07-08 12:32:42,490 - INFO - --- Dask Config (relevant parts) ---
2025-07-08 12:32:42,490 - INFO - distributed.comm.compression: False
2025-07-08 12:32:42,490 - INFO - --- End Environment Info ---
2025-07-08 12:32:42,491 - distributed.core - INFO - Event loop was unresponsive in Worker for 14.55s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-07-08 12:33:43,669 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 23.23 GiB -- Worker memory limit: 27.94 GiB
2025-07-08 12:33:44,471 - distributed.nanny.memory - WARNING - Worker tcp://10.110.100.15:38927 (pid=3821576) exceeded 95% memory budget. Restarting...
2025-07-08 12:33:44,581 - distributed.nanny - INFO - Worker process 3821576 was killed by signal 15
2025-07-08 12:33:44,592 - distributed.nanny - WARNING - Restarting worker
2025-07-08 12:33:55,170 - distributed.worker - INFO -       Start worker at:  tcp://10.110.100.15:46317
2025-07-08 12:33:55,170 - distributed.worker - INFO -          Listening to:  tcp://10.110.100.15:46317
2025-07-08 12:33:55,170 - distributed.worker - INFO -           Worker name:               SGECluster-0
2025-07-08 12:33:55,170 - distributed.worker - INFO -          dashboard at:        10.110.100.15:40555
2025-07-08 12:33:55,170 - distributed.worker - INFO - Waiting to connect to:  tcp://10.110.100.15:35189
2025-07-08 12:33:55,172 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:33:55,172 - distributed.worker - INFO -               Threads:                          2
2025-07-08 12:33:55,172 - distributed.worker - INFO -                Memory:                  27.94 GiB
2025-07-08 12:33:55,172 - distributed.worker - INFO -       Local Directory: /tmp/18930494.1.beliveau-long.q/dask-scratch-space/worker-41jwlisr
2025-07-08 12:33:55,172 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:33:59,008 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-07-08 12:33:59,009 - distributed.worker - INFO -         Registered to:  tcp://10.110.100.15:35189
2025-07-08 12:33:59,009 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:33:59,009 - distributed.core - INFO - Starting established connection to tcp://10.110.100.15:35189
2025-07-08 12:34:06,650 - distributed.core - INFO - Event loop was unresponsive in Worker for 6.99s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-07-08 12:34:15,927 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 22.88 GiB -- Worker memory limit: 27.94 GiB
2025-07-08 12:34:16,871 - distributed.nanny.memory - WARNING - Worker tcp://10.110.100.15:46317 (pid=3821790) exceeded 95% memory budget. Restarting...
2025-07-08 12:34:16,955 - distributed.nanny - INFO - Worker process 3821790 was killed by signal 15
2025-07-08 12:34:16,965 - distributed.nanny - WARNING - Restarting worker
2025-07-08 12:34:22,962 - distributed.worker - INFO -       Start worker at:  tcp://10.110.100.15:35301
2025-07-08 12:34:22,962 - distributed.worker - INFO -          Listening to:  tcp://10.110.100.15:35301
2025-07-08 12:34:22,962 - distributed.worker - INFO -           Worker name:               SGECluster-0
2025-07-08 12:34:22,962 - distributed.worker - INFO -          dashboard at:        10.110.100.15:33749
2025-07-08 12:34:22,962 - distributed.worker - INFO - Waiting to connect to:  tcp://10.110.100.15:35189
2025-07-08 12:34:22,962 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:34:22,962 - distributed.worker - INFO -               Threads:                          2
2025-07-08 12:34:22,963 - distributed.worker - INFO -                Memory:                  27.94 GiB
2025-07-08 12:34:22,963 - distributed.worker - INFO -       Local Directory: /tmp/18930494.1.beliveau-long.q/dask-scratch-space/worker-29zus0nw
2025-07-08 12:34:22,963 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:34:25,093 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-07-08 12:34:25,093 - distributed.worker - INFO -         Registered to:  tcp://10.110.100.15:35189
2025-07-08 12:34:25,093 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:34:25,094 - distributed.core - INFO - Starting established connection to tcp://10.110.100.15:35189
2025-07-08 12:34:28,856 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.41s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-07-08 12:34:37,952 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 23.18 GiB -- Worker memory limit: 27.94 GiB
2025-07-08 12:34:38,871 - distributed.nanny.memory - WARNING - Worker tcp://10.110.100.15:35301 (pid=3821920) exceeded 95% memory budget. Restarting...
2025-07-08 12:34:38,958 - distributed.nanny - INFO - Worker process 3821920 was killed by signal 15
2025-07-08 12:34:38,970 - distributed.nanny - WARNING - Restarting worker
2025-07-08 12:34:47,179 - distributed.worker - INFO -       Start worker at:  tcp://10.110.100.15:37967
2025-07-08 12:34:47,179 - distributed.worker - INFO -          Listening to:  tcp://10.110.100.15:37967
2025-07-08 12:34:47,179 - distributed.worker - INFO -           Worker name:               SGECluster-0
2025-07-08 12:34:47,179 - distributed.worker - INFO -          dashboard at:        10.110.100.15:43759
2025-07-08 12:34:47,179 - distributed.worker - INFO - Waiting to connect to:  tcp://10.110.100.15:35189
2025-07-08 12:34:47,179 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:34:47,179 - distributed.worker - INFO -               Threads:                          2
2025-07-08 12:34:47,180 - distributed.worker - INFO -                Memory:                  27.94 GiB
2025-07-08 12:34:47,180 - distributed.worker - INFO -       Local Directory: /tmp/18930494.1.beliveau-long.q/dask-scratch-space/worker-g2yk2wu3
2025-07-08 12:34:47,180 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:34:50,266 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-07-08 12:34:50,267 - distributed.worker - INFO -         Registered to:  tcp://10.110.100.15:35189
2025-07-08 12:34:50,267 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:34:50,267 - distributed.core - INFO - Starting established connection to tcp://10.110.100.15:35189
2025-07-08 12:34:55,774 - distributed.core - INFO - Event loop was unresponsive in Worker for 5.04s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-07-08 12:35:05,027 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 22.66 GiB -- Worker memory limit: 27.94 GiB
2025-07-08 12:35:05,971 - distributed.nanny.memory - WARNING - Worker tcp://10.110.100.15:37967 (pid=3822018) exceeded 95% memory budget. Restarting...
2025-07-08 12:35:06,055 - distributed.nanny - INFO - Worker process 3822018 was killed by signal 15
2025-07-08 12:35:06,141 - distributed.nanny - WARNING - Restarting worker
2025-07-08 12:35:13,304 - distributed.worker - INFO -       Start worker at:  tcp://10.110.100.15:38099
2025-07-08 12:35:13,305 - distributed.worker - INFO -          Listening to:  tcp://10.110.100.15:38099
2025-07-08 12:35:13,305 - distributed.worker - INFO -           Worker name:               SGECluster-0
2025-07-08 12:35:13,305 - distributed.worker - INFO -          dashboard at:        10.110.100.15:37645
2025-07-08 12:35:13,305 - distributed.worker - INFO - Waiting to connect to:  tcp://10.110.100.15:35189
2025-07-08 12:35:13,305 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:35:13,305 - distributed.worker - INFO -               Threads:                          2
2025-07-08 12:35:13,305 - distributed.worker - INFO -                Memory:                  27.94 GiB
2025-07-08 12:35:13,305 - distributed.worker - INFO -       Local Directory: /tmp/18930494.1.beliveau-long.q/dask-scratch-space/worker-5lve1nsc
2025-07-08 12:35:13,305 - distributed.worker - INFO - -------------------------------------------------
2025-07-08 12:35:43,306 - distributed.worker - INFO - Waiting to connect to:  tcp://10.110.100.15:35189
2025-07-08 12:36:06,142 - distributed.nanny - ERROR - Timed out connecting Nanny '<Nanny: tcp://10.110.100.15:37967, threads: 2>' to scheduler 'tcp://10.110.100.15:35189'
2025-07-08 12:36:06,142 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.110.100.15:32851'. Reason: nanny-instantiate-timeout
2025-07-08 12:36:06,142 - distributed.nanny - ERROR - Failed to restart worker after its process exited
Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 896, in _wait_until_connected
    msg = self.init_result_q.get_nowait()
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/multiprocessing/queues.py", line 133, in get_nowait
    return self.get(False)
           ~~~~~~~~^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/multiprocessing/queues.py", line 114, in get
    raise Empty
_queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/utils.py", line 1910, in wait_for
    return await fut
           ^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 759, in start
    msg = await self._wait_until_connected(uid)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 898, in _wait_until_connected
    await asyncio.sleep(self._init_msg_interval)
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/asyncio/tasks.py", line 718, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 438, in instantiate
    result = await wait_for(self.process.start(), self.death_timeout)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/utils.py", line 1909, in wait_for
    async with asyncio.timeout(timeout):
               ~~~~~~~~~~~~~~~^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/asyncio/timeouts.py", line 116, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 569, in _on_worker_exit
    await self.instantiate()
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 445, in instantiate
    await self.close(
        timeout=self.death_timeout, reason="nanny-instantiate-timeout"
    )
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 619, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 400, in kill
    await self.process.kill(reason=reason, timeout=timeout)
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 845, in kill
    assert self.status in (
           ^^^^^^^^^^^^^^^^
    ...<5 lines>...
    ), self.status
    ^
AssertionError: Status.starting
2025-07-08 12:36:11,451 - distributed.worker - INFO - Stopping worker at tcp://10.110.100.15:38099. Reason: failure-to-start-<class 'TimeoutError'>
2025-07-08 12:36:11,451 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2025-07-08 12:36:11,453 - distributed.nanny - ERROR - Failed to start worker
ConnectionRefusedError: [Errno 111] Connection refused

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
           ^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/utils.py", line 1910, in wait_for
    return await fut
           ^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/comm/tcp.py", line 560, in connect
    convert_stream_closed_error(self, e)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc.__class__.__name__}: {exc}") from exc
distributed.comm.core.CommClosedError: in <distributed.comm.tcp.TCPConnector object at 0x15293054efd0>: ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/utils.py", line 1910, in wait_for
    return await fut
           ^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/worker.py", line 1497, in start_unsafe
    await self._register_with_scheduler()
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/worker.py", line 1196, in _register_with_scheduler
    comm = await connect(self.scheduler.address, **self.connection_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/asyncio/tasks.py", line 718, in sleep
    return await future
           ^^^^^^^^^^^^
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/core.py", line 528, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/utils.py", line 1909, in wait_for
    async with asyncio.timeout(timeout):
               ~~~~~~~~~~~~~~~^^^^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/asyncio/timeouts.py", line 116, in __aexit__
    raise TimeoutError from exc_val
TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/nanny.py", line 969, in run
    async with worker:
               ^^^^^^
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/core.py", line 542, in __aenter__
    await self
  File "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/core.py", line 531, in start
    raise asyncio.TimeoutError(
        f"{type(self).__name__} start timed out after {timeout}s."
    ) from exc
TimeoutError: Worker start timed out after 60s.
2025-07-08 12:36:13,478 - distributed.nanny - ERROR - Worker process died unexpectedly
