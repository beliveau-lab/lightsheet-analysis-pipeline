{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 10:47:17,627 - INFO - --- Environment Versions ---\n",
      "2025-07-10 10:47:17,628 - INFO - Platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\n",
      "2025-07-10 10:47:17,629 - INFO - Python: 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:03) [GCC 13.3.0]\n",
      "2025-07-10 10:47:17,629 - INFO - Dask: 2025.2.0\n",
      "2025-07-10 10:47:17,655 - INFO - Distributed: 2025.2.0\n",
      "2025-07-10 10:47:17,656 - INFO - Cloudpickle: 3.0.0\n",
      "2025-07-10 10:47:17,657 - INFO - Msgpack: 1.0.8\n",
      "2025-07-10 10:47:17,657 - INFO - Zarr: 2.13.3\n",
      "2025-07-10 10:47:17,658 - INFO - NumPy: 2.2.6\n",
      "2025-07-10 10:47:17,659 - INFO - Scikit-image: 0.25.0\n",
      "2025-07-10 10:47:17,659 - INFO - --- Dask Config (relevant parts) ---\n",
      "2025-07-10 10:47:17,660 - INFO - distributed.comm.compression: False\n",
      "2025-07-10 10:47:17,661 - INFO - --- End Environment Info ---\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.measure import regionprops_table\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import feature_extraction as fe\n",
    "from aicsshparam import shtools, shparam # spherical harmonics package\n",
    "import seaborn as sns\n",
    "import sparse\n",
    "import align_3d as align\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sparse\n",
    "import dask.array as da\n",
    "from aicsshparam import shtools, shparam\n",
    "from dask import delayed, compute\n",
    "\n",
    "from utils.dask_utils import setup_dask_sge_cluster, shutdown_dask\n",
    "from dask.distributed import Client, LocalCluster, get_client\n",
    "\n",
    "mask_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused_masks_cpsamr5.zarr'\n",
    "n5_image_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused.n5'\n",
    "\n",
    "# def load_all_data(mask_path, \n",
    "#                   n5_path, \n",
    "#                   crop_size=None,\n",
    "#                   subset_size=None):\n",
    "#     # load mask and image data\n",
    "#     mask_da = fe.load_n5_zarr_array(mask_path)\n",
    "#     image_da = fe.load_n5_zarr_array(n5_path, n5_subpath='ch0/s0')\n",
    "\n",
    "#     if crop_size is not None:\n",
    "#         mask_da = mask_da[:crop_size[0], :crop_size[1], :crop_size[2]]\n",
    "#         image_da = image_da[:crop_size[0], :crop_size[1], :crop_size[2]]\n",
    "\n",
    "#     # convert to sparse\n",
    "#     chunk_shape = tuple(c[0] for c in mask_da.chunks)\n",
    "#     meta_block = sparse.COO.from_numpy(np.zeros(chunk_shape, \n",
    "#                                                 dtype=mask_da.dtype))\n",
    "#     mask_sparse = mask_da.map_blocks(\n",
    "#     fe.to_sparse,\n",
    "#     dtype=mask_da.dtype,\n",
    "#     meta=meta_block,\n",
    "#     chunks=mask_da.chunks\n",
    "#     )\n",
    "\n",
    "#     # find bounding boxes\n",
    "#     df_bboxes = fe.find_objects(mask_sparse).compute()\n",
    "#     df_bboxes = pd.DataFrame(df_bboxes)\n",
    "#     print(f\"Found {len(df_bboxes)} objects\")\n",
    "\n",
    "#     if subset_size is not None:\n",
    "#         obj_idxs = np.random.randint(0, \n",
    "#                                      len(df_bboxes), \n",
    "#                                      size=subset_size)\n",
    "#         test_objects = df_bboxes.iloc[obj_idxs]\n",
    "#         print(df_bboxes.head(3))\n",
    "#         return mask_da, image_da, test_objects\n",
    "#     else: \n",
    "#         print(df_bboxes.head(3))\n",
    "#         return mask_da, image_da, df_bboxes\n",
    "# mask_da, image_da, df_bboxes = load_all_data(mask_path, n5_image_path, crop_size=(800, 800, 800), subset_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from dask import delayed, compute\n",
    "import sparse\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Import your modules\n",
    "import align_3d as align\n",
    "import feature_extraction as fe\n",
    "from aicsshparam import shtools, shparam\n",
    "from utils.dask_utils import setup_dask_sge_cluster, shutdown_dask\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_LMAX = 12\n",
    "MIN_RELIABLE_OBJECTS = 30\n",
    "\n",
    "#TODO: add a n > p check\n",
    "\n",
    "def process_individual(obj, mask_da, min_size):\n",
    "    \"\"\"Process a single object in distributed manner.\"\"\"\n",
    "    try:\n",
    "        slice_z, slice_y, slice_x = obj[0], obj[1], obj[2]\n",
    "        obj_id = int(obj.name)\n",
    "        \n",
    "        mask_slice = mask_da[slice_z, slice_y, slice_x].compute()\n",
    "        label_slice = np.where(mask_slice == obj_id, mask_slice, 0)\n",
    "        \n",
    "        if np.sum(label_slice > 0) < min_size:\n",
    "            logger.debug(f\"Object {obj_id} is too small\")\n",
    "            return None\n",
    "            \n",
    "        aligned_obj, _ = align.align_object(label_slice, {})\n",
    "        return {'obj_id': obj_id, 'aligned_obj': aligned_obj}\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Failed to process object {obj.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def precompute_errors(object_batch, lmax_range):\n",
    "    \"\"\"Compute both reconstruction errors and coefficients for a batch of objects.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for obj_data in object_batch:\n",
    "        if obj_data is None:\n",
    "            continue\n",
    "            \n",
    "        obj_id = obj_data['obj_id']\n",
    "        aligned_obj = obj_data['aligned_obj']\n",
    "        obj_results = {'errors': {}, 'coeffs': {}}\n",
    "        \n",
    "        for lmax in lmax_range:\n",
    "            try:\n",
    "                # Get coefficients and reconstruction in one call\n",
    "                (_, grid_rec), (_, _, grid, _) = shparam.get_shcoeffs(\n",
    "                    aligned_obj, lmax=lmax, alignment_2d=False\n",
    "                )\n",
    "                # Compute reconstruction error\n",
    "                mse = shtools.get_reconstruction_error(grid, grid_rec)\n",
    "                if mse is not None and not np.isnan(mse):\n",
    "                    obj_results['errors'][lmax] = mse                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed computation for object {obj_id} lmax {lmax}: {e}\")\n",
    "        \n",
    "        if obj_results['errors']:  # If we got at least one valid result\n",
    "            results[obj_id] = obj_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class BootstrapLmaxSelection:\n",
    "    \"\"\"\n",
    "    Leave-One-Out Cross-Validation for selecting optimal lmax for spherical harmonics\n",
    "    decomposition of 3D objects from a mouse brain sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.client = None\n",
    "        self.cluster = None\n",
    "        self.lmax_range = list(range(self.params['lmax_min'], \n",
    "                                     self.params['lmax_max'], \n",
    "                                     4))\n",
    "        \n",
    "    def select_optimal_lmax(self, mask_path):\n",
    "        \"\"\"\n",
    "        Main entry point: finds optimal lmax for this mouse sample.\n",
    "        \n",
    "        Args:\n",
    "            mask_path: Path to segmentation mask\n",
    "            n5_path: Path to n5 image data (optional, not used currently)\n",
    "        \n",
    "        Returns:\n",
    "            optimal_lmax: Best lmax value\n",
    "            cv_results: Cross-validation results for each lmax\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Setup distributed computing\n",
    "            if self.params.get('use_dask', True):\n",
    "                self._setup_dask()\n",
    "            \n",
    "            # Load and process objects\n",
    "            mask_da, df_bboxes = self._load_data(mask_path)\n",
    "            processed_objects = self._process_objects_vanilla(mask_da, df_bboxes)\n",
    "            \n",
    "            if len(processed_objects) < MIN_RELIABLE_OBJECTS:\n",
    "                logger.warning(f\"Only {len(processed_objects)} valid objects found. Results may be unreliable.\")\n",
    "            \n",
    "            # Compute coefficients and errors for all lmax values\n",
    "            all_data = self._precompute_all(processed_objects)\n",
    "\n",
    "            # bootstrap_lmax = self.bootstrap_lmax_selection(all_data, n_bootstrap=100)\n",
    "            # aic_lmax = self.aic_lmax_selection(all_data, k_folds=5)\n",
    "            # return bootstrap_lmax, aic_lmax\n",
    "            \n",
    "        finally:\n",
    "            # Always cleanup\n",
    "            if self.client:\n",
    "                self._shutdown_dask()\n",
    "    \n",
    "    def _load_data(self, mask_path):\n",
    "        \"\"\"Load objects from mask.\"\"\"\n",
    "        logger.info(\"Loading data and finding objects...\")\n",
    "        mask_da = fe.load_n5_zarr_array(mask_path)\n",
    "        \n",
    "        # Find objects using sparse representation\n",
    "        chunk_shape = tuple(c[0] for c in mask_da.chunks)\n",
    "        meta_block = sparse.COO.from_numpy(np.zeros(chunk_shape, dtype=mask_da.dtype))\n",
    "        mask_sparse = mask_da.map_blocks(\n",
    "            fe.to_sparse, \n",
    "            dtype=mask_da.dtype, \n",
    "            meta=meta_block, \n",
    "            chunks=mask_da.chunks\n",
    "        )\n",
    "        df_bboxes = fe.find_objects(mask_sparse).compute()\n",
    "        df_bboxes = pd.DataFrame(df_bboxes)\n",
    "        logger.info(f\"Found {len(df_bboxes)} total objects\")\n",
    "        return mask_da, df_bboxes\n",
    "    \n",
    "    def _process_objects_vanilla(self, mask_da, df_bboxes):\n",
    "        n_sample = min(self.params['sample_size'], len(df_bboxes))\n",
    "        sample = df_bboxes.sample(n_sample)\n",
    "        logger.info(f\"Sampled {len(sample)} objects\")\n",
    "\n",
    "        all_objects = []\n",
    "        for _, obj in sample.iterrows():\n",
    "            obj_data = process_individual(obj, mask_da, self.params['min_object_size'])\n",
    "            if obj_data is not None:\n",
    "                all_objects.append(obj_data)\n",
    "            else: \n",
    "                logger.info(\"failed to process object\")\n",
    "        if len(all_objects) == 0:\n",
    "            logger.debug(\"No valid objects found\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(all_objects)} valid objects\")\n",
    "        return all_objects\n",
    "    \n",
    "    \n",
    "    # def _process_objects_batch(self, mask_da, df_bboxes):\n",
    "    #     \"\"\"Sample and create delayed processing tasks for objects.\"\"\"\n",
    "    #     logger.info(\"Creating delayed tasks for object pre-processing...\")\n",
    "        \n",
    "    #     n_sample = min(self.params['sample_size'], len(df_bboxes))\n",
    "    #     sample = df_bboxes.sample(n_sample, random_state=42)  # Add random_state for reproducibility\n",
    "    #     logger.info(f\"Sampled {len(sample)} objects\")\n",
    "        \n",
    "    #     # Create delayed tasks for object processing\n",
    "    #     delayed_tasks = []\n",
    "    #     for _, obj in sample.iterrows():\n",
    "    #         delayed_task = delayed(process_individual)(\n",
    "    #             obj, mask_da, self.params['min_object_size']\n",
    "    #         )\n",
    "    #         delayed_tasks.append(delayed_task)\n",
    "        \n",
    "    #     logger.info(f\"Created {len(delayed_tasks)} delayed object processing tasks.\")\n",
    "    #     return delayed_tasks\n",
    "    \n",
    "    def _precompute_all(self, processed_objects):\n",
    "        \"\"\"Compute coefficients and errors for all objects and lmax values.\"\"\"\n",
    "        lmax_range = range(self.params['lmax_min'], self.params['lmax_max'] + 1)\n",
    "        logger.info(\"Computing spherical harmonics data...\")\n",
    "        \n",
    "        # Create delayed tasks for batches\n",
    "        batch_size = self.params.get('error_batch_size', 20)\n",
    "        delayed_tasks = []\n",
    "        \n",
    "        for i in range(0, len(processed_objects), batch_size):\n",
    "            batch = processed_objects[i:i + batch_size]\n",
    "            task = delayed(precompute_errors)(batch, lmax_range)\n",
    "            delayed_tasks.append(task)\n",
    "        \n",
    "        # Execute all tasks\n",
    "        logger.info(f\"Computing data for {len(delayed_tasks)} batches...\")\n",
    "        batch_results = compute(*delayed_tasks)\n",
    "        \n",
    "        # Combine results\n",
    "        all_data = {}\n",
    "        for batch_result in batch_results:\n",
    "            all_data.update(batch_result)\n",
    "        \n",
    "        logger.info(f\"Computed data for {len(all_data)} objects\")\n",
    "        print(all_data)\n",
    "        return all_data\n",
    "   \n",
    "    def bootstrap_lmax_selection(self, all_data, n_bootstrap=100):\n",
    "        \"\"\"Fixed bootstrap selection.\"\"\"\n",
    "        \n",
    "        # Get valid object IDs\n",
    "        valid_objects = list(all_data.keys())\n",
    "        \n",
    "        if len(valid_objects) < 30:\n",
    "            logger.warning(\"Too few objects for reliable bootstrap\")\n",
    "        \n",
    "        lmax_stats = {}\n",
    "        \n",
    "        for lmax in self.lmax_range:\n",
    "            bootstrap_errors = []\n",
    "            \n",
    "            # Get objects that have data for this lmax\n",
    "            objects_with_lmax = [\n",
    "                obj_id for obj_id in valid_objects \n",
    "                if lmax in all_data[obj_id]['errors']\n",
    "            ]\n",
    "            \n",
    "            if len(objects_with_lmax) < 10:\n",
    "                logger.warning(f\"Too few objects for lmax {lmax}\")\n",
    "                continue\n",
    "                \n",
    "            for _ in range(n_bootstrap):\n",
    "                # Correct bootstrap sampling\n",
    "                bootstrap_sample = np.random.choice(\n",
    "                    objects_with_lmax, \n",
    "                    size=len(objects_with_lmax), \n",
    "                    replace=True\n",
    "                )\n",
    "                \n",
    "                sample_errors = []\n",
    "                for obj_id in bootstrap_sample:\n",
    "                    error = all_data[obj_id]['errors'][lmax]\n",
    "                    sample_errors.append(error)\n",
    "                \n",
    "                bootstrap_errors.append(np.mean(sample_errors))\n",
    "            \n",
    "            lmax_stats[lmax] = {\n",
    "                'mean_error': np.mean(bootstrap_errors),\n",
    "                'std_error': np.std(bootstrap_errors),\n",
    "                'n_objects': len(objects_with_lmax)\n",
    "            }\n",
    "        \n",
    "        # Still problematic: just chooses lowest error\n",
    "            return min(lmax_stats.keys(), key=lambda k: lmax_stats[k]['mean_error'])\n",
    "    \n",
    "    def aic_lmax_selection(self, all_data, k_folds=5):\n",
    "        \"\"\"Statistically sound lmax selection using cross-validated AIC.\"\"\"\n",
    "        \n",
    "        valid_objects = list(all_data.keys())\n",
    "        fold_size = len(valid_objects) // k_folds\n",
    "        \n",
    "        lmax_aic_scores = {}\n",
    "        \n",
    "        for lmax in self.lmax_range:\n",
    "            fold_scores = []\n",
    "            n_coeffs = (lmax + 1) ** 2\n",
    "            \n",
    "            # K-fold cross-validation\n",
    "            for fold in range(k_folds):\n",
    "                start_idx = fold * fold_size\n",
    "                end_idx = (fold + 1) * fold_size if fold < k_folds - 1 else len(valid_objects)\n",
    "                \n",
    "                test_objects = valid_objects[start_idx:end_idx]\n",
    "                test_errors = []\n",
    "                \n",
    "                for obj_id in test_objects:\n",
    "                    if lmax in all_data[obj_id]['errors']:\n",
    "                        test_errors.append(all_data[obj_id]['errors'][lmax])\n",
    "                \n",
    "                if len(test_errors) > 0:\n",
    "                    mean_error = np.mean(test_errors)\n",
    "                    # AIC formula\n",
    "                    aic = len(test_errors) * np.log(mean_error) + 2 * n_coeffs\n",
    "                    fold_scores.append(aic)\n",
    "            \n",
    "            if fold_scores:\n",
    "                lmax_aic_scores[lmax] = np.mean(fold_scores)\n",
    "        \n",
    "        # Choose lmax with minimum AIC (best trade-off)\n",
    "        optimal_lmax = min(lmax_aic_scores.keys(), key=lambda k: lmax_aic_scores[k])\n",
    "        \n",
    "        logger.info(f\"AIC scores: {lmax_aic_scores}\")\n",
    "        logger.info(f\"Selected optimal lmax: {optimal_lmax}\")\n",
    "        \n",
    "        return optimal_lmax, lmax_aic_scores\n",
    "    def _setup_dask(self):\n",
    "        \"\"\"Setup Dask SGE cluster for distributed processing.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Setting up distributed Dask cluster...\")\n",
    "            self.cluster, self.client = setup_dask_sge_cluster(\n",
    "                n_workers=self.params.get('num_workers', 8),\n",
    "                cores=self.params.get('cores_per_worker', 2),\n",
    "                processes=self.params.get('processes', 1),\n",
    "                memory=self.params.get('mem_per_worker', '30G'),\n",
    "                project=self.params.get('project', 'beliveaulab'),\n",
    "                queue=self.params.get('queue', 'beliveau-long.q'),\n",
    "                runtime=self.params.get('runtime', '7200'),\n",
    "                resource_spec=self.params.get('resource_spec', 'mfree=30G'),\n",
    "                log_directory=self.params.get('log_dir', None),\n",
    "                conda_env=self.params.get('conda_env', 'otls-pipeline')\n",
    "            )\n",
    "            logger.info(f\"Dask dashboard link: {self.client.dashboard_link}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup distributed cluster: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _shutdown_dask(self):\n",
    "        \"\"\"Shutdown Dask cluster.\"\"\"\n",
    "        if self.cluster and self.client:\n",
    "            try:\n",
    "                shutdown_dask(self.cluster, self.client)\n",
    "                logger.info(\"Dask cluster shut down successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error shutting down cluster: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# loocv_optimizer = SphericalHarmonicsLOOCV(params)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# optimal_lmax, cv_results = loocv_optimizer.select_optimal_lmax(mask_path)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# print(f\"Optimal lmax: {optimal_lmax}\")\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# print(\"CV results:\", cv_results)\u001b[39;00m\n\u001b[32m     22\u001b[39m optimizer = BootstrapLmaxSelection(params)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbootstrap_lmax_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 214\u001b[39m, in \u001b[36mBootstrapLmaxSelection.bootstrap_lmax_selection\u001b[39m\u001b[34m(self, all_data, n_bootstrap)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fixed bootstrap selection.\"\"\"\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Get valid object IDs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m valid_objects = \u001b[38;5;28mlist\u001b[39m(\u001b[43mall_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m())\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_objects) < \u001b[32m30\u001b[39m:\n\u001b[32m    217\u001b[39m     logger.warning(\u001b[33m\"\u001b[39m\u001b[33mToo few objects for reliable bootstrap\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"num_workers\": 1,\n",
    "    \"mem_per_worker\": \"64G\",\n",
    "    \"cores_per_worker\": 2,\n",
    "    \"sample_size\": 100,\n",
    "    \"sampling_rate\": 2.752,\n",
    "    \"min_object_size\": 100,\n",
    "    \"lmax_min\": 4,\n",
    "    \"lmax_max\": 28,\n",
    "    \"default_lmax\": 12,\n",
    "    \"error_batch_size\": 25,# Process 25 objects at a time per dask task,\n",
    "    'use_dask': False\n",
    "}\n",
    "mask_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused_masks_cpsamr5.zarr'\n",
    "n5_image_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused.n5'\n",
    "\n",
    "# loocv_optimizer = SphericalHarmonicsLOOCV(params)\n",
    "# optimal_lmax, cv_results = loocv_optimizer.select_optimal_lmax(mask_path)\n",
    "# print(f\"Optimal lmax: {optimal_lmax}\")\n",
    "# print(\"CV results:\", cv_results)\n",
    "\n",
    "optimizer = BootstrapLmaxSelection(params)\n",
    "optimizer.bootstrap_lmax_selection(mask_path)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otls-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
