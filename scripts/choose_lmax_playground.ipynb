{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 10:51:46,434 - INFO - Successfuly imported local module\n",
      "2025-07-07 10:51:46,436 - INFO - --- Environment Versions ---\n",
      "2025-07-07 10:51:46,438 - INFO - Platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\n",
      "2025-07-07 10:51:46,439 - INFO - Python: 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:03) [GCC 13.3.0]\n",
      "2025-07-07 10:51:46,440 - INFO - Dask: 2025.2.0\n",
      "2025-07-07 10:51:46,443 - INFO - Distributed: 2025.2.0\n",
      "2025-07-07 10:51:46,444 - INFO - Cloudpickle: 3.0.0\n",
      "2025-07-07 10:51:46,445 - INFO - Msgpack: 1.0.8\n",
      "2025-07-07 10:51:46,447 - INFO - Zarr: 2.13.3\n",
      "2025-07-07 10:51:46,448 - INFO - NumPy: 2.2.6\n",
      "2025-07-07 10:51:46,450 - INFO - Scikit-image: 0.25.0\n",
      "2025-07-07 10:51:46,451 - INFO - --- Dask Config (relevant parts) ---\n",
      "2025-07-07 10:51:46,452 - INFO - distributed.comm.compression: False\n",
      "2025-07-07 10:51:46,453 - INFO - --- End Environment Info ---\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.measure import regionprops_table\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import feature_extraction as fe\n",
    "from aicsshparam import shtools, shparam # spherical harmonics package\n",
    "import seaborn as sns\n",
    "import sparse\n",
    "import align_3d as align\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sparse\n",
    "import dask.array as da\n",
    "from aicsshparam import shtools, shparam\n",
    "from dask import delayed, compute\n",
    "\n",
    "from utils.dask_utils import setup_dask_sge_cluster, shutdown_dask\n",
    "from dask.distributed import Client, LocalCluster, get_client\n",
    "\n",
    "mask_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused_masks_cpsamr5.zarr'\n",
    "n5_image_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused.n5'\n",
    "\n",
    "# def load_all_data(mask_path, \n",
    "#                   n5_path, \n",
    "#                   crop_size=None,\n",
    "#                   subset_size=None):\n",
    "#     # load mask and image data\n",
    "#     mask_da = fe.load_n5_zarr_array(mask_path)\n",
    "#     image_da = fe.load_n5_zarr_array(n5_path, n5_subpath='ch0/s0')\n",
    "\n",
    "#     if crop_size is not None:\n",
    "#         mask_da = mask_da[:crop_size[0], :crop_size[1], :crop_size[2]]\n",
    "#         image_da = image_da[:crop_size[0], :crop_size[1], :crop_size[2]]\n",
    "\n",
    "#     # convert to sparse\n",
    "#     chunk_shape = tuple(c[0] for c in mask_da.chunks)\n",
    "#     meta_block = sparse.COO.from_numpy(np.zeros(chunk_shape, \n",
    "#                                                 dtype=mask_da.dtype))\n",
    "#     mask_sparse = mask_da.map_blocks(\n",
    "#     fe.to_sparse,\n",
    "#     dtype=mask_da.dtype,\n",
    "#     meta=meta_block,\n",
    "#     chunks=mask_da.chunks\n",
    "#     )\n",
    "\n",
    "#     # find bounding boxes\n",
    "#     df_bboxes = fe.find_objects(mask_sparse).compute()\n",
    "#     df_bboxes = pd.DataFrame(df_bboxes)\n",
    "#     print(f\"Found {len(df_bboxes)} objects\")\n",
    "\n",
    "#     if subset_size is not None:\n",
    "#         obj_idxs = np.random.randint(0, \n",
    "#                                      len(df_bboxes), \n",
    "#                                      size=subset_size)\n",
    "#         test_objects = df_bboxes.iloc[obj_idxs]\n",
    "#         print(df_bboxes.head(3))\n",
    "#         return mask_da, image_da, test_objects\n",
    "#     else: \n",
    "#         print(df_bboxes.head(3))\n",
    "#         return mask_da, image_da, df_bboxes\n",
    "# mask_da, image_da, df_bboxes = load_all_data(mask_path, n5_image_path, crop_size=(800, 800, 800), subset_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 12:31:49,333 - INFO - Successfuly imported local module\n",
      "2025-07-08 12:31:49,334 - INFO - --- Environment Versions ---\n",
      "2025-07-08 12:31:49,335 - INFO - Platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\n",
      "2025-07-08 12:31:49,336 - INFO - Python: 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:03) [GCC 13.3.0]\n",
      "2025-07-08 12:31:49,337 - INFO - Dask: 2025.2.0\n",
      "2025-07-08 12:31:49,339 - INFO - Distributed: 2025.2.0\n",
      "2025-07-08 12:31:49,340 - INFO - Cloudpickle: 3.0.0\n",
      "2025-07-08 12:31:49,340 - INFO - Msgpack: 1.0.8\n",
      "2025-07-08 12:31:49,341 - INFO - Zarr: 2.13.3\n",
      "2025-07-08 12:31:49,341 - INFO - NumPy: 2.2.6\n",
      "2025-07-08 12:31:49,343 - INFO - Scikit-image: 0.25.0\n",
      "2025-07-08 12:31:49,343 - INFO - --- Dask Config (relevant parts) ---\n",
      "2025-07-08 12:31:49,344 - INFO - distributed.comm.compression: False\n",
      "2025-07-08 12:31:49,345 - INFO - --- End Environment Info ---\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from dask import delayed, compute\n",
    "import sparse\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "\n",
    "# Import your modules\n",
    "import align_3d as align\n",
    "import feature_extraction as fe\n",
    "from aicsshparam import shtools, shparam\n",
    "from utils.dask_utils import setup_dask_sge_cluster, shutdown_dask\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "DEFAULT_LMAX = 12\n",
    "MIN_RELIABLE_OBJECTS = 30\n",
    "\n",
    "#TODO: add a n > p check\n",
    "\n",
    "def process_individual(obj, mask_da, min_size):\n",
    "    \"\"\"Process a single object in distributed manner.\"\"\"\n",
    "    try:\n",
    "        slice_z, slice_y, slice_x = obj[0], obj[1], obj[2]\n",
    "        obj_id = int(obj.name)\n",
    "        \n",
    "        mask_slice = mask_da[slice_z, slice_y, slice_x].compute()\n",
    "        label_slice = np.where(mask_slice == obj_id, mask_slice, 0)\n",
    "        \n",
    "        if np.sum(label_slice > 0) < min_size:\n",
    "            logger.debug(f\"Object {obj_id} is too small\")\n",
    "            return None\n",
    "            \n",
    "        aligned_obj, _ = align.align_object(label_slice, {})\n",
    "        return {'obj_id': obj_id, 'aligned_obj': aligned_obj}\n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Failed to process object {obj.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_coeffs_and_errors(object_batch, lmax_range):\n",
    "    \"\"\"Compute both reconstruction errors and coefficients for a batch of objects.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for obj_data in object_batch:\n",
    "        if obj_data is None:\n",
    "            continue\n",
    "            \n",
    "        obj_id = obj_data['obj_id']\n",
    "        aligned_obj = obj_data['aligned_obj']\n",
    "        obj_results = {'errors': {}, 'coeffs': {}}\n",
    "        \n",
    "        for lmax in lmax_range:\n",
    "            try:\n",
    "                # Get coefficients and reconstruction in one call\n",
    "                (coeffs_dict, grid_rec), (_, _, grid, _) = shparam.get_shcoeffs(\n",
    "                    aligned_obj, lmax=lmax, alignment_2d=False\n",
    "                )\n",
    "                \n",
    "                # Compute reconstruction error\n",
    "                mse = shtools.get_reconstruction_error(grid, grid_rec)\n",
    "                \n",
    "                if mse is not None and not np.isnan(mse):\n",
    "                    obj_results['errors'][lmax] = mse\n",
    "                    obj_results['coeffs'][lmax] = coeffs_dict\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Failed computation for object {obj_id} lmax {lmax}: {e}\")\n",
    "        \n",
    "        if obj_results['errors']:  # If we got at least one valid result\n",
    "            results[obj_id] = obj_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class SphericalHarmonicsLOOCV:\n",
    "    \"\"\"\n",
    "    Leave-One-Out Cross-Validation for selecting optimal lmax for spherical harmonics\n",
    "    decomposition of 3D objects from a mouse brain sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.client = None\n",
    "        self.cluster = None\n",
    "        self.lmax_range = list(range(self.params['lmax_min'], \n",
    "                                     self.params['lmax_max'], \n",
    "                                     4))\n",
    "        \n",
    "    def select_optimal_lmax(self, mask_path):\n",
    "        \"\"\"\n",
    "        Main entry point: finds optimal lmax for this mouse sample.\n",
    "        \n",
    "        Args:\n",
    "            mask_path: Path to segmentation mask\n",
    "            n5_path: Path to n5 image data (optional, not used currently)\n",
    "        \n",
    "        Returns:\n",
    "            optimal_lmax: Best lmax value\n",
    "            cv_results: Cross-validation results for each lmax\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Setup distributed computing\n",
    "            if self.params.get('use_dask', True):\n",
    "                self._setup_dask()\n",
    "            \n",
    "            # Load and process objects\n",
    "            mask_da, df_bboxes = self._load_data(mask_path)\n",
    "            processed_objects = self._process_objects_batch(mask_da, df_bboxes)\n",
    "            \n",
    "            if len(processed_objects) < MIN_RELIABLE_OBJECTS:\n",
    "                logger.warning(f\"Only {len(processed_objects)} valid objects found. Results may be unreliable.\")\n",
    "            \n",
    "            # Compute coefficients and errors for all lmax values\n",
    "            all_data = self._precompute_all(processed_objects)\n",
    "            \n",
    "            # Run LOOCV evaluation\n",
    "            cv_results = self._evaluate_lmax_loocv(all_data)\n",
    "            \n",
    "            # Find optimal lmax\n",
    "            optimal_lmax = self._find_optimal_lmax(cv_results)\n",
    "            \n",
    "            return optimal_lmax, cv_results\n",
    "            \n",
    "        finally:\n",
    "            # Always cleanup\n",
    "            if self.client:\n",
    "                self._shutdown_dask()\n",
    "    \n",
    "    def _load_data(self, mask_path):\n",
    "        \"\"\"Load objects from mask.\"\"\"\n",
    "        logger.info(\"Loading data and finding objects...\")\n",
    "        mask_da = fe.load_n5_zarr_array(mask_path)\n",
    "        \n",
    "        # Find objects using sparse representation\n",
    "        chunk_shape = tuple(c[0] for c in mask_da.chunks)\n",
    "        meta_block = sparse.COO.from_numpy(np.zeros(chunk_shape, dtype=mask_da.dtype))\n",
    "        mask_sparse = mask_da.map_blocks(\n",
    "            fe.to_sparse, \n",
    "            dtype=mask_da.dtype, \n",
    "            meta=meta_block, \n",
    "            chunks=mask_da.chunks\n",
    "        )\n",
    "        df_bboxes = fe.find_objects(mask_sparse).compute()\n",
    "        df_bboxes = pd.DataFrame(df_bboxes)\n",
    "        logger.info(f\"Found {len(df_bboxes)} total objects\")\n",
    "        return mask_da, df_bboxes\n",
    "    \n",
    "    def _process_objects_vanilla(self, mask_da, df_bboxes):\n",
    "        n_sample = min(self.params['sample_size'], len(df_bboxes))\n",
    "        sample = df_bboxes.sample(n_sample)\n",
    "        logger.info(f\"Sampled {len(sample)} objects\")\n",
    "\n",
    "        all_objects = []\n",
    "        for _, obj in sample.iterrows():\n",
    "            obj_data = process_individual(obj, mask_da, self.params['min_object_size'])\n",
    "            if obj_data is not None:\n",
    "                all_objects.append(obj_data)\n",
    "            else: \n",
    "                logger.info(\"failed to process object\")\n",
    "        if len(all_objects) == 0:\n",
    "            logger.debug(\"No valid objects found\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(all_objects)} valid objects\")\n",
    "        return all_objects\n",
    "    \n",
    "    \n",
    "    def _process_objects_batch(self, mask_da, df_bboxes):\n",
    "        \"\"\"Sample and create delayed processing tasks for objects.\"\"\"\n",
    "        logger.info(\"Creating delayed tasks for object pre-processing...\")\n",
    "        \n",
    "        n_sample = min(self.params['sample_size'], len(df_bboxes))\n",
    "        sample = df_bboxes.sample(n_sample, random_state=42)  # Add random_state for reproducibility\n",
    "        logger.info(f\"Sampled {len(sample)} objects\")\n",
    "        \n",
    "        # Create delayed tasks for object processing\n",
    "        delayed_tasks = []\n",
    "        for _, obj in sample.iterrows():\n",
    "            delayed_task = delayed(process_individual)(\n",
    "                obj, mask_da, self.params['min_object_size']\n",
    "            )\n",
    "            delayed_tasks.append(delayed_task)\n",
    "        \n",
    "        logger.info(f\"Created {len(delayed_tasks)} delayed object processing tasks.\")\n",
    "        return delayed_tasks\n",
    "    \n",
    "    def _precompute_all(self, processed_objects):\n",
    "        \"\"\"Compute coefficients and errors for all objects and lmax values.\"\"\"\n",
    "        lmax_range = range(self.params['lmax_min'], self.params['lmax_max'] + 1)\n",
    "        logger.info(\"Computing spherical harmonics data...\")\n",
    "        \n",
    "        # Create delayed tasks for batches\n",
    "        batch_size = self.params.get('error_batch_size', 20)\n",
    "        delayed_tasks = []\n",
    "        \n",
    "        for i in range(0, len(processed_objects), batch_size):\n",
    "            batch = processed_objects[i:i + batch_size]\n",
    "            task = delayed(get_coeffs_and_errors)(batch, lmax_range)\n",
    "            delayed_tasks.append(task)\n",
    "        \n",
    "        # Execute all tasks\n",
    "        logger.info(f\"Computing data for {len(delayed_tasks)} batches...\")\n",
    "        batch_results = compute(*delayed_tasks)\n",
    "        \n",
    "        # Combine results\n",
    "        all_data = {}\n",
    "        for batch_result in batch_results:\n",
    "            all_data.update(batch_result)\n",
    "        \n",
    "        logger.info(f\"Computed data for {len(all_data)} objects\")\n",
    "        return all_data\n",
    "    \n",
    "    def _evaluate_lmax_loocv(self, all_data):\n",
    "        \"\"\"Evaluate each lmax using closed form LOOCV.\"\"\"\n",
    "        cv_results = {}\n",
    "        for lmax in self.lmax_range:\n",
    "            logger.info(f\"Evaluating lmax={lmax}\")\n",
    "            \n",
    "            # Extract residuals and coefficients for this lmax\n",
    "            residuals = []\n",
    "            coeffs_dicts = []\n",
    "            \n",
    "            for obj_id, obj_data in all_data.items():\n",
    "                if lmax in obj_data['errors'] and lmax in obj_data['coeffs']:\n",
    "                    # Apply sampling rate to convert to physical units\n",
    "                    error = obj_data['errors'][lmax] * self.params.get('sampling_rate', 2.752)\n",
    "                    residuals.append(error)\n",
    "                    coeffs_dicts.append(obj_data['coeffs'][lmax])\n",
    "            \n",
    "            # Build design matrix\n",
    "            X = self._build_design_matrix(coeffs_dicts, lmax)\n",
    "            \n",
    "            # Compute LOOCV error\n",
    "            residuals = np.array(residuals)\n",
    "            cv_error = self._compute_loocv_error(residuals, X)\n",
    "            \n",
    "            cv_results[lmax] = {\n",
    "                'cv_error': cv_error,\n",
    "                'n_objects': len(residuals),\n",
    "                'mean_residual': np.mean(residuals),\n",
    "                'std_residual': np.std(residuals)\n",
    "            }\n",
    "            logger.info(f\"  LOOCV error: {cv_error:.6f}, mean residual: {np.mean(residuals):.6f}\")\n",
    "        return cv_results\n",
    "    \n",
    "    def _get_coeff_mapping(self, lmax):\n",
    "        \"\"\"Create mapping from coefficient names to column indices\"\"\"\"\"\n",
    "        col_map = {}\n",
    "        col_idx = 0\n",
    "        for L in range(lmax + 1):\n",
    "            for M in range(L + 1):\n",
    "                if M == 0:\n",
    "                    col_map[f\"shcoeffs_L{L}M{M}C\"] = col_idx\n",
    "                    col_idx += 1\n",
    "                else:\n",
    "                    col_map[f\"shcoeffs_L{L}M{M}C\"] = col_idx\n",
    "                    col_map[f\"shcoeffs_L{L}M{M}S\"] = col_idx + 1\n",
    "                    col_idx += 2\n",
    "        return col_map\n",
    "    \n",
    "    def _build_design_matrix(self, coeffs_dicts, lmax):\n",
    "        \"\"\"Build design matrix from coefficient dictionaries.\"\"\"\n",
    "        n = len(coeffs_dicts)\n",
    "        p = (lmax + 1) ** 2\n",
    "        if n <= p:\n",
    "            raise ValueError(f\"n <= p: {n} <= {p}. Insufficient data to compute design matrix. Please decrease lmax range or increase sample size.\")\n",
    "        \n",
    "        # Create mapping from coefficient names to column indices\n",
    "        col_map = self._get_coeff_mapping(lmax)\n",
    "\n",
    "        # Build sparse matrix\n",
    "        rows, cols, data = [], [], []\n",
    "        for i, coeffs_dict in enumerate(coeffs_dicts):\n",
    "            for key, value in coeffs_dict.items():\n",
    "                if key in col_map and abs(value) > 1e-10:\n",
    "                    rows.append(i)\n",
    "                    cols.append(col_map[key])\n",
    "                    data.append(value)\n",
    "        \n",
    "        # Create sparse matrix and convert to dense\n",
    "        X_sparse = csr_matrix((data, (rows, cols)), shape=(n, p))\n",
    "        return X_sparse\n",
    "    \n",
    "    def _compute_hat_diagonal(self, X_sparse, XtX_inv):\n",
    "        n_samples = X_sparse.shape[0]\n",
    "        hat_diagonal = np.zeros(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            x_i = X_sparse.getrow(i).toarray().flatten()  # Get single row\n",
    "            hat_diagonal[i] = x_i @ XtX_inv @ x_i  # Scalar computation\n",
    "        return hat_diagonal\n",
    "    \n",
    "    def _compute_loocv_error(self, residuals, X_sparse):\n",
    "        \"\"\"Compute LOOCV error using closed-form solution.\"\"\"\n",
    "        try:\n",
    "            # Hat matrix computation: H = X(X'X)^{-1}X'\n",
    "            XtX = X_sparse.T @ X_sparse\n",
    "            XtX_dense = XtX.toarray()\n",
    "            # Compute (X.T @ X)^{-1}\n",
    "            XtX_inv = np.linalg.pinv(XtX_dense) # Use pseudo-inverse for stability\n",
    "            \n",
    "            # Compute diagonal of hat matrix efficiently\n",
    "            # H_diag = np.sum(X * (X @ XtX_inv), axis=1)\n",
    "            H_diag = self._compute_hat_diagonal(X_sparse, XtX_inv)\n",
    "            \n",
    "            # Check numerical stability\n",
    "            if np.any(H_diag >= 1.0 - 1e-10):\n",
    "                logger.warning(\"Numerical instability detected in hat matrix\")\n",
    "                return float('inf')\n",
    "            \n",
    "            # LOOCV residuals\n",
    "            loocv_residuals = residuals / (1 - H_diag)\n",
    "            cv_error = np.mean(loocv_residuals ** 2)\n",
    "            \n",
    "            return cv_error\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in LOOCV computation: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def _find_optimal_lmax(self, cv_results):\n",
    "        \"\"\"Find optimal lmax from CV results.\"\"\"\n",
    "        # Filter valid results\n",
    "        valid_results = {\n",
    "            lmax: res['cv_error'] \n",
    "            for lmax, res in cv_results.items() \n",
    "            if res['cv_error'] != float('inf')\n",
    "        }\n",
    "        \n",
    "        if not valid_results:\n",
    "            logger.warning(f\"No valid CV results, using default lmax: {DEFAULT_LMAX}\")\n",
    "            return DEFAULT_LMAX\n",
    "        \n",
    "        # Find minimum error\n",
    "        optimal_lmax = min(valid_results, key=valid_results.get)\n",
    "        logger.info(f\"Optimal lmax: {optimal_lmax} with CV error: {valid_results[optimal_lmax]:.6f}\")\n",
    "        return optimal_lmax\n",
    "    \n",
    "    def _setup_dask(self):\n",
    "        \"\"\"Setup Dask SGE cluster for distributed processing.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Setting up distributed Dask cluster...\")\n",
    "            self.cluster, self.client = setup_dask_sge_cluster(\n",
    "                n_workers=self.params.get('num_workers', 8),\n",
    "                cores=self.params.get('cores_per_worker', 2),\n",
    "                processes=self.params.get('processes', 1),\n",
    "                memory=self.params.get('mem_per_worker', '30G'),\n",
    "                project=self.params.get('project', 'beliveaulab'),\n",
    "                queue=self.params.get('queue', 'beliveau-long.q'),\n",
    "                runtime=self.params.get('runtime', '7200'),\n",
    "                resource_spec=self.params.get('resource_spec', 'mfree=30G'),\n",
    "                log_directory=self.params.get('log_dir', None),\n",
    "                conda_env=self.params.get('conda_env', 'otls-pipeline')\n",
    "            )\n",
    "            logger.info(f\"Dask dashboard link: {self.client.dashboard_link}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup distributed cluster: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _shutdown_dask(self):\n",
    "        \"\"\"Shutdown Dask cluster.\"\"\"\n",
    "        if self.cluster and self.client:\n",
    "            try:\n",
    "                shutdown_dask(self.cluster, self.client)\n",
    "                logger.info(\"Dask cluster shut down successfully\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error shutting down cluster: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 12:31:49,359 - INFO - Setting up distributed Dask cluster...\n",
      "2025-07-08 12:31:49,360 - INFO - Setting up Dask SGE Cluster...\n",
      "2025-07-08 12:31:49,360 - INFO - Dask config applied: {'temporary-directory': '/tmp/17965644.1.beliveau-long.q', 'distributed.comm.timeouts.connect': '3600s', 'distributed.comm.timeouts.tcp': '3600s', 'distributed.worker.memory.spill': 0.7, 'distributed.worker.memory.pause': 0.9, 'distributed.worker.memory.terminate': 0.98, 'distributed.scheduler.work-stealing': True, 'distributed.scheduler.worker-saturation': 1.1}\n",
      "2025-07-08 12:31:49,362 - INFO - Dask worker logs will be stored in: ./dask_worker_logs_20250708_123149\n",
      "2025-07-08 12:31:49,362 - INFO - Using project: beliveaulab, queue: beliveau-long.q\n",
      "2025-07-08 12:31:49,363 - INFO - Requesting 2 cores, 1 processes, 30G memory per worker.\n",
      "2025-07-08 12:31:49,364 - INFO - Job extra directives: ['-P beliveaulab -pe serial 2', '-l mfree=30G']\n",
      "2025-07-08 12:31:52,633 - INFO - Configured SGECluster. Requesting 1 workers.\n",
      "2025-07-08 12:31:52,638 - INFO - Scaling cluster...\n",
      "2025-07-08 12:31:52,639 - INFO - Cluster scale command issued. Waiting for workers (check dashboard).\n",
      "2025-07-08 12:32:15,423 - INFO - Successfully connected to 1 workers.\n",
      "2025-07-08 12:32:15,424 - INFO - Dask dashboard available at: http://10.110.100.15:8787/status\n",
      "2025-07-08 12:32:15,424 - INFO - Dask dashboard link: http://10.110.100.15:8787/status\n",
      "2025-07-08 12:32:15,425 - INFO - Loading data and finding objects...\n",
      "2025-07-08 12:32:15,425 - INFO - Attempting to load from: /net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused_masks_cpsamr5.zarr\n",
      "2025-07-08 12:32:15,428 - INFO - Loaded Zarr array: Shape=(896, 1013, 2687)\n",
      "2025-07-08 12:33:34,575 - INFO - Found 9553 total objects\n",
      "2025-07-08 12:33:34,576 - INFO - Creating delayed tasks for object pre-processing...\n",
      "2025-07-08 12:33:34,580 - INFO - Sampled 100 objects\n",
      "2025-07-08 12:33:34,597 - INFO - Created 100 delayed object processing tasks.\n",
      "2025-07-08 12:33:34,600 - INFO - Computing spherical harmonics data...\n",
      "2025-07-08 12:33:34,601 - INFO - Computing data for 4 batches...\n",
      "2025-07-08 12:35:06,054 - distributed.scheduler - ERROR - Task finalize-c9d86bcc-4d9a-4b59-927e-a36116c2a15c marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,055 - distributed.scheduler - ERROR - Task finalize-d6962d50-a9ac-4875-9be3-66661774a923 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,056 - distributed.scheduler - ERROR - Task finalize-eabfcb88-3bc0-4c2c-a8fc-0f12dfa380de marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,057 - distributed.scheduler - ERROR - Task finalize-2cc4fe62-28f2-44ce-8e82-2e8cf467a23b marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,058 - distributed.scheduler - ERROR - Task finalize-6442b294-46f4-4f51-9e31-cefbd0f0237a marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,059 - distributed.scheduler - ERROR - Task finalize-7da17693-b8bb-4f54-b177-913e590b0376 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,059 - distributed.scheduler - ERROR - Task finalize-500ee348-c15f-4504-a980-3c96a570b38c marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,060 - distributed.scheduler - ERROR - Task finalize-14b646be-701b-4e56-9bda-6a7a2c092622 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,061 - distributed.scheduler - ERROR - Task finalize-bc340a29-72f7-4e95-b0fe-79d0dbbf4e1c marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,061 - distributed.scheduler - ERROR - Task finalize-04607dc2-806e-43dd-8808-83df31b10f5c marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,062 - distributed.scheduler - ERROR - Task finalize-2dfc2d70-4319-436b-ae6e-6b4bdb8be06a marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,063 - distributed.scheduler - ERROR - Task finalize-f96fb656-5907-44d8-8584-68d306cea5f7 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,064 - distributed.scheduler - ERROR - Task finalize-e28009a2-ceb6-4494-9c03-94b44bad2b84 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,064 - distributed.scheduler - ERROR - Task finalize-55308aa9-de1f-4268-aabf-0ceb2fe46c2b marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,065 - distributed.scheduler - ERROR - Task finalize-67a9dc19-e912-4f75-a7b3-d65fbb5a78a6 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,066 - distributed.scheduler - ERROR - Task finalize-e7d499c8-3c54-497b-8843-89b0189c1a62 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,067 - distributed.scheduler - ERROR - Task finalize-3b4a4ed0-e721-4a35-b5f4-e95d53816097 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,068 - distributed.scheduler - ERROR - Task finalize-231dd1fa-4e03-45af-9f62-b2590387f149 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,069 - distributed.scheduler - ERROR - Task finalize-1812a343-6cd4-4cdc-aa5e-161bddb2026b marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,069 - distributed.scheduler - ERROR - Task finalize-024063ca-9230-4309-95b3-e8fd9fca238d marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,070 - distributed.scheduler - ERROR - Task finalize-b2acfd3a-32e6-45be-84fa-70da58be5047 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,071 - distributed.scheduler - ERROR - Task finalize-5e1fb306-b52d-4baf-8ab7-721040865766 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,072 - distributed.scheduler - ERROR - Task finalize-e4ea70e9-4a95-4965-a6cc-e57e8096db86 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,073 - distributed.scheduler - ERROR - Task finalize-20fc3031-1537-4574-bcd4-821d041af7b8 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,073 - distributed.scheduler - ERROR - Task finalize-b723fd32-01bd-497f-908a-2e694adbc969 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,074 - distributed.scheduler - ERROR - Task finalize-818dfcd8-de00-4fd6-b0f2-abc4ad13f63e marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,075 - distributed.scheduler - ERROR - Task finalize-d3835fc2-d438-4f6a-b1df-557fddaaaaaa marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,075 - distributed.scheduler - ERROR - Task finalize-34463cf8-1f74-4251-bee9-25144e1ce082 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,076 - distributed.scheduler - ERROR - Task finalize-73df4ce5-46ac-4338-bd92-ac73a45bcd2d marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,077 - distributed.scheduler - ERROR - Task finalize-3873a476-1557-4502-b226-9a8d64219a8b marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,078 - distributed.scheduler - ERROR - Task finalize-949512a7-e21b-4a55-b1f3-ceffdfffba0a marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,079 - distributed.scheduler - ERROR - Task finalize-32557755-a459-4819-8802-7c4e0a292b19 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,080 - distributed.scheduler - ERROR - Task finalize-6d0228b6-555a-4304-8899-7f7d872d369d marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,080 - distributed.scheduler - ERROR - Task finalize-f2c3829e-eb03-41d6-b475-cb5230cfd0cf marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,081 - distributed.scheduler - ERROR - Task finalize-37956cea-173c-4de8-ba80-a40e5c827ecc marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,082 - distributed.scheduler - ERROR - Task finalize-d16f3bd8-c000-4f09-a339-b0eda4a6689a marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,083 - distributed.scheduler - ERROR - Task finalize-58ed6a05-d42d-406f-94cd-8a7407139577 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,084 - distributed.scheduler - ERROR - Task finalize-08d050e5-a291-40e0-adfd-f1378cb92856 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,085 - distributed.scheduler - ERROR - Task finalize-9e44b932-99d4-4754-8675-97fe04c7d62f marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,085 - distributed.scheduler - ERROR - Task finalize-4acb1aaa-8995-4ce1-a0e8-6c3e404b9598 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,086 - distributed.scheduler - ERROR - Task finalize-aaee9610-bdcb-4b22-a2cb-6ca204afbb97 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,087 - distributed.scheduler - ERROR - Task finalize-bd312005-3a3e-4cd9-965b-f1f66015a2fb marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,088 - distributed.scheduler - ERROR - Task finalize-d60cb77d-c6c6-4865-bff2-3c142d01346d marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,089 - distributed.scheduler - ERROR - Task finalize-4aa3d9bc-be10-432e-989e-8e9c4841af27 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,091 - distributed.scheduler - ERROR - Task finalize-c401cf20-fab6-4642-8502-dbe2730539c0 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,091 - distributed.scheduler - ERROR - Task finalize-e5ffafa0-736a-44d0-afce-8e52503186c0 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,092 - distributed.scheduler - ERROR - Task finalize-5292ff06-d11f-45ea-9caf-4798a5da15e2 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,093 - distributed.scheduler - ERROR - Task finalize-6608e3e2-c11f-4d2a-8e87-7d8e2f74a3ec marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,094 - distributed.scheduler - ERROR - Task finalize-81ff8c4f-d462-41cf-a525-32cf3cd4f4e3 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,094 - distributed.scheduler - ERROR - Task finalize-b8296b9d-43d8-4ec7-abac-cc8ddd9e71d6 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,095 - distributed.scheduler - ERROR - Task finalize-c96f2d53-2692-46fb-9cd1-891a29a726e4 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,096 - distributed.scheduler - ERROR - Task finalize-355b21ad-4e42-407e-b98a-595454b3e1a1 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,096 - distributed.scheduler - ERROR - Task finalize-798a9120-7e73-409e-975f-93ca1847c1c7 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,097 - distributed.scheduler - ERROR - Task finalize-bad17ee1-5479-4fcc-84f2-aeb0ed86d859 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,098 - distributed.scheduler - ERROR - Task finalize-56e33c5e-25e6-46cc-b310-f213ea340b24 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,098 - distributed.scheduler - ERROR - Task finalize-f01caa08-cac7-4e41-8d8c-65d0065eb4eb marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,100 - distributed.scheduler - ERROR - Task finalize-91b0f107-1569-4f95-90e7-10b1d4b615a1 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,100 - distributed.scheduler - ERROR - Task finalize-629a3758-a94c-4040-be26-6edde1362362 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,101 - distributed.scheduler - ERROR - Task finalize-0e95d5d3-a4c9-4490-a13b-86c331770126 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,102 - distributed.scheduler - ERROR - Task finalize-9a34a03e-c251-4f50-a40d-39bcbbd77c43 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,103 - distributed.scheduler - ERROR - Task finalize-b4415b72-20fd-466a-adea-b6f7fde9b0b2 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,103 - distributed.scheduler - ERROR - Task finalize-cc8d7156-8327-4d72-b848-c0b1c6688e7b marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,104 - distributed.scheduler - ERROR - Task finalize-036acf51-c606-43b6-bd38-e445a3ea5b10 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,105 - distributed.scheduler - ERROR - Task finalize-fc899a99-d804-4a9c-8e22-48a4d73eb2c9 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,106 - distributed.scheduler - ERROR - Task finalize-7994d37a-f1ae-47e4-83c9-1598b6f2aea8 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,106 - distributed.scheduler - ERROR - Task finalize-3351d64a-dfad-49e0-8b94-31a8abde7a0d marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,107 - distributed.scheduler - ERROR - Task finalize-a3995e72-caad-4e8c-99de-7c4dcd5826e9 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,108 - distributed.scheduler - ERROR - Task finalize-fd836d6d-417e-46ed-9611-08ab1d88b950 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,109 - distributed.scheduler - ERROR - Task finalize-9fbdfa29-1d88-41b3-847c-238999bbfbe9 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,110 - distributed.scheduler - ERROR - Task finalize-60646979-511e-49aa-b84d-c1d0db1c69be marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,110 - distributed.scheduler - ERROR - Task finalize-5280c654-b988-4f7f-9ee1-a47084563af2 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,111 - distributed.scheduler - ERROR - Task finalize-ce65be0b-80ef-4a53-ab4f-70993fcfe574 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,112 - distributed.scheduler - ERROR - Task finalize-7532003a-e3ae-429a-9003-db9b10c664ba marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,112 - distributed.scheduler - ERROR - Task finalize-0ca4a7b2-e426-4067-899f-9fd2eaa1febc marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,113 - distributed.scheduler - ERROR - Task finalize-7da547d0-dc47-4b6e-a60c-d1f34226aee9 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,114 - distributed.scheduler - ERROR - Task finalize-490cf66c-7a13-47f4-8949-52f1b5dec4aa marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,114 - distributed.scheduler - ERROR - Task finalize-78463997-8a00-494a-80cb-def76c227517 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,115 - distributed.scheduler - ERROR - Task finalize-3e6642e7-8dbc-49a2-a055-8df72274c0bd marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,116 - distributed.scheduler - ERROR - Task finalize-4b65066e-0e5e-47b8-b789-7e515f48c2f4 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,116 - distributed.scheduler - ERROR - Task finalize-d13028df-2d60-488c-9648-728dde059098 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,117 - distributed.scheduler - ERROR - Task finalize-5be9aa33-f2a9-46c0-8feb-0f71870d6af6 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,118 - distributed.scheduler - ERROR - Task finalize-0e562392-1a92-4ebe-8f1a-08924e2402b7 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,118 - distributed.scheduler - ERROR - Task finalize-d9246fcd-23ce-424a-a469-2a4e38b4d9c5 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,119 - distributed.scheduler - ERROR - Task finalize-28c546a2-73dc-4461-adfd-2fc3d61139ac marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,122 - distributed.scheduler - ERROR - Task finalize-87d1d9e5-2a2b-4e0b-89a4-64889fed4fac marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,122 - distributed.scheduler - ERROR - Task finalize-00b0bb03-3a2a-4677-b732-2d284f38fc44 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,123 - distributed.scheduler - ERROR - Task finalize-fcae55aa-2d01-4492-b3ca-bb8f3dd33389 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,124 - distributed.scheduler - ERROR - Task finalize-124278f4-624f-4ba5-ae4f-ed7f346b8506 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,125 - distributed.scheduler - ERROR - Task finalize-5e353046-eeb8-4c1a-b0b6-34945f8d21da marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,125 - distributed.scheduler - ERROR - Task finalize-644ff9c7-86c8-44df-a8de-6f6760d901ea marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,126 - distributed.scheduler - ERROR - Task finalize-7db32398-0c3a-4193-b6e4-7089264be483 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,131 - distributed.scheduler - ERROR - Task finalize-5efd77ac-5c6e-4fe1-9988-8f750191c56b marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,132 - distributed.scheduler - ERROR - Task finalize-69a07adb-5fb0-4459-beb4-f084f07098ca marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,133 - distributed.scheduler - ERROR - Task finalize-3f5f7ce9-c8b9-4e9b-bf28-eb507caa6432 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,134 - distributed.scheduler - ERROR - Task finalize-a39d3876-1086-49a8-8343-a3d115aa940a marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,135 - distributed.scheduler - ERROR - Task finalize-d87837f4-efc5-49c7-8176-6c686eacc4f6 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,136 - distributed.scheduler - ERROR - Task finalize-089a6686-0833-4309-8151-d10b63cf2d64 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,136 - distributed.scheduler - ERROR - Task finalize-d38b6062-d11c-4fb1-a3d4-bf73fb89c690 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,137 - distributed.scheduler - ERROR - Task finalize-a5f15783-8a3d-4bbe-b65f-b8e9fc9dae2b marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,138 - distributed.scheduler - ERROR - Task finalize-dcabd351-7fa5-4a35-a1b3-b526477084a1 marked as failed because 4 workers died while trying to run it\n",
      "2025-07-08 12:35:06,145 - INFO - Shutting down Dask client and cluster...\n",
      "2025-07-08 12:35:06,158 - INFO - Dask client closed.\n",
      "2025-07-08 12:35:06,167 - INFO - Dask cluster closed.\n",
      "2025-07-08 12:35:06,168 - INFO - Dask shutdown complete.\n",
      "2025-07-08 12:35:06,169 - INFO - Dask cluster shut down successfully\n"
     ]
    },
    {
     "ename": "KilledWorker",
     "evalue": "Attempted to run task 'finalize-124278f4-624f-4ba5-ae4f-ed7f346b8506' on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://10.110.100.15:37967. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKilledWorker\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m n5_image_path = \u001b[33m'\u001b[39m\u001b[33m/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused.n5\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     16\u001b[39m loocv_optimizer = SphericalHarmonicsLOOCV(params)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m optimal_lmax, cv_results = \u001b[43mloocv_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect_optimal_lmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimal lmax: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_lmax\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCV results:\u001b[39m\u001b[33m\"\u001b[39m, cv_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mSphericalHarmonicsLOOCV.select_optimal_lmax\u001b[39m\u001b[34m(self, mask_path)\u001b[39m\n\u001b[32m    115\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(processed_objects)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m valid objects found. Results may be unreliable.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Compute coefficients and errors for all lmax values\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m all_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_precompute_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_objects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Run LOOCV evaluation\u001b[39;00m\n\u001b[32m    121\u001b[39m cv_results = \u001b[38;5;28mself\u001b[39m._evaluate_lmax_loocv(all_data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 207\u001b[39m, in \u001b[36mSphericalHarmonicsLOOCV._precompute_all\u001b[39m\u001b[34m(self, processed_objects)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Execute all tasks\u001b[39;00m\n\u001b[32m    206\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing data for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(delayed_tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m batches...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m batch_results = \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdelayed_tasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Combine results\u001b[39;00m\n\u001b[32m    210\u001b[39m all_data = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/dask/base.py:662\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     postcomputes.append(x.__dask_postcompute__())\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     results = \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, *a) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/distributed/client.py:2426\u001b[39m, in \u001b[36mClient._gather\u001b[39m\u001b[34m(self, futures, errors, direct, local_worker)\u001b[39m\n\u001b[32m   2424\u001b[39m     exception = st.exception\n\u001b[32m   2425\u001b[39m     traceback = st.traceback\n\u001b[32m-> \u001b[39m\u001b[32m2426\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception.with_traceback(traceback)\n\u001b[32m   2427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2428\u001b[39m     bad_keys.add(key)\n",
      "\u001b[31mKilledWorker\u001b[39m: Attempted to run task 'finalize-124278f4-624f-4ba5-ae4f-ed7f346b8506' on 4 different workers, but all those workers died while running it. The last worker that attempt to run the task was tcp://10.110.100.15:37967. Inspecting worker logs is often a good next step to diagnose what went wrong. For more information see https://distributed.dask.org/en/stable/killed.html."
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"num_workers\": 1,\n",
    "    \"mem_per_worker\": \"64G\",\n",
    "    \"cores_per_worker\": 2,\n",
    "    \"sample_size\": 100,\n",
    "    \"sampling_rate\": 2.752,\n",
    "    \"min_object_size\": 100,\n",
    "    \"lmax_min\": 4,\n",
    "    \"lmax_max\": 28,\n",
    "    \"default_lmax\": 12,\n",
    "    \"error_batch_size\": 25 # Process 25 objects at a time per dask task\n",
    "}\n",
    "mask_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused_masks_cpsamr5.zarr'\n",
    "n5_image_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused.n5'\n",
    "\n",
    "loocv_optimizer = SphericalHarmonicsLOOCV(params)\n",
    "optimal_lmax, cv_results = loocv_optimizer.select_optimal_lmax(mask_path)\n",
    "print(f\"Optimal lmax: {optimal_lmax}\")\n",
    "print(\"CV results:\", cv_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otls-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
