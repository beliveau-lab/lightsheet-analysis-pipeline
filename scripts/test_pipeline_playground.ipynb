{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 10:28:40,236 - INFO - --- Environment Versions ---\n",
      "2025-07-10 10:28:40,236 - INFO - Platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\n",
      "2025-07-10 10:28:40,237 - INFO - Python: 3.13.3 | packaged by conda-forge | (main, Apr 14 2025, 20:44:03) [GCC 13.3.0]\n",
      "2025-07-10 10:28:40,238 - INFO - Dask: 2025.2.0\n",
      "2025-07-10 10:28:40,240 - INFO - Distributed: 2025.2.0\n",
      "2025-07-10 10:28:40,241 - INFO - Cloudpickle: 3.0.0\n",
      "2025-07-10 10:28:40,241 - INFO - Msgpack: 1.0.8\n",
      "2025-07-10 10:28:40,242 - INFO - Zarr: 2.13.3\n",
      "2025-07-10 10:28:40,243 - INFO - NumPy: 2.2.6\n",
      "2025-07-10 10:28:40,243 - INFO - Scikit-image: 0.25.0\n",
      "2025-07-10 10:28:40,244 - INFO - --- Dask Config (relevant parts) ---\n",
      "2025-07-10 10:28:40,245 - INFO - distributed.comm.compression: False\n",
      "2025-07-10 10:28:40,246 - INFO - --- End Environment Info ---\n",
      "2025-07-10 10:28:40,247 - INFO - Attempting to load from: /net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused_masks_cpsamr5.zarr\n",
      "2025-07-10 10:28:40,251 - INFO - Loaded Zarr array: Shape=(896, 1013, 2687)\n",
      "2025-07-10 10:28:40,253 - INFO - Attempting to load from: /net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused.n5 with N5 subpath: ch0/s0\n",
      "2025-07-10 10:28:40,257 - INFO - Loaded N5 array: Shape=(896, 1013, 2687), Chunks=(512, 512, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 976 objects\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import dask.array as da\n",
    "from skimage.measure import regionprops_table\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import feature_extraction as fe\n",
    "import sparse\n",
    "\n",
    "\n",
    "mask_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused_masks_cpsamr5.zarr'\n",
    "n5_image_path = '/net/beliveau/vol2/instrument/E9.5_290/Zoom_290_subset_test/dataset_fused.n5'\n",
    "\n",
    "def load_all_data(mask_path, \n",
    "                  n5_path, \n",
    "                  do_crop=False,\n",
    "                  subset_size=None):\n",
    "    # load mask and image data\n",
    "    mask_da = fe.load_n5_zarr_array(mask_path)\n",
    "    image_da = fe.load_n5_zarr_array(n5_path, n5_subpath='ch0/s0')\n",
    "\n",
    "    if do_crop:\n",
    "        crop_size = (800, 800, 800)  # z, y, x\n",
    "        mask_da = mask_da[:crop_size[0], :crop_size[1], :crop_size[2]]\n",
    "        image_da = image_da[:crop_size[0], :crop_size[1], :crop_size[2]]\n",
    "\n",
    "    # convert to sparse\n",
    "    chunk_shape = tuple(c[0] for c in mask_da.chunks)\n",
    "    meta_block = sparse.COO.from_numpy(np.zeros(chunk_shape, \n",
    "                                                dtype=mask_da.dtype))\n",
    "    mask_sparse = mask_da.map_blocks(\n",
    "    fe.to_sparse,\n",
    "    dtype=mask_da.dtype,\n",
    "    meta=meta_block,\n",
    "    chunks=mask_da.chunks\n",
    "    )\n",
    "\n",
    "    # find bounding boxes\n",
    "    df_bboxes = fe.find_objects(mask_sparse).compute()\n",
    "    df_bboxes = pd.DataFrame(df_bboxes)\n",
    "    print(f\"Found {len(df_bboxes)} objects\")\n",
    "\n",
    "    if subset_size is not None:\n",
    "        obj_idxs = np.random.randint(0, \n",
    "                                     len(df_bboxes), \n",
    "                                     size=subset_size)\n",
    "        test_objects = df_bboxes.iloc[obj_idxs]\n",
    "        return mask_da, image_da, test_objects\n",
    "    return mask_da, image_da \n",
    "\n",
    "mask_da, image_da, test_objects = load_all_data(mask_path=mask_path,\n",
    "                                                n5_path=n5_image_path,\n",
    "                                                do_crop=True,\n",
    "                                                subset_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Define enhanced process_object function\n",
    "import align_3d as align\n",
    "import plot_utils\n",
    "from importlib import reload\n",
    "reload(align)\n",
    "reload(plot_utils)\n",
    "# import warnings\n",
    "from scipy.ndimage import center_of_mass\n",
    "# warnings.simplefilter(\"always\")\n",
    "from aicsshparam import shparam\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def process_object(obj, mask_da, image_da, lmax):\n",
    "    obj_id = int(obj.name)\n",
    "    slice_z = obj[0]\n",
    "    slice_y = obj[1] \n",
    "    slice_x = obj[2]\n",
    "\n",
    "    # Compute centroid of object (in global coordinates)\n",
    "    centroid_z = slice_z.start + (slice_z.stop - slice_z.start) // 2\n",
    "    centroid_y = slice_y.start + (slice_y.stop - slice_y.start) // 2\n",
    "    centroid_x = slice_x.start + (slice_x.stop - slice_x.start) // 2\n",
    "\n",
    "    try:\n",
    "        # Get only the pixels belonging to the object of interest\n",
    "        label_slice = np.where(mask_da == obj_id, mask_da, 0)\n",
    "        # Basic regionprops\n",
    "        props = regionprops_table(label_slice, image_da, properties=[\"label\", \n",
    "                                                                     \"area\", \n",
    "                                                                     \"mean_intensity\"])\n",
    "        area = props['area'][0]\n",
    "        if area < 4000:  # Increase threshold and fix indexing\n",
    "            # logger.info(f\"Object {obj_id} too small (area={area}). Skipping...\")\n",
    "            return None\n",
    "        # else:\n",
    "        #     logger.info(f\"Object {obj_id} is large enough (area={area}). Processing...\")\n",
    "\n",
    "        # Add global centroid coordinates\n",
    "        props['centroid_z'] = centroid_z\n",
    "        props['centroid_y'] = centroid_y\n",
    "        props['centroid_x'] = centroid_x\n",
    "        # Align object\n",
    "        aligned_slice, props = align.align_object(label_slice, props) # align object also adds features to df_props\n",
    "        \n",
    "        # # Check if aligned object has any foreground voxels\n",
    "        # if np.sum(aligned_slice) == 0:\n",
    "        #     logger.warning(f\"Object {obj_id} has no foreground voxels after alignment. Skipping spherical harmonics...\")\n",
    "        #     return None\n",
    "            \n",
    "        # Spherical harmonics computation\n",
    "        (coeffs, _), _ = shparam.get_shcoeffs(\n",
    "            aligned_slice, \n",
    "            lmax=lmax,\n",
    "            alignment_2d=False)\n",
    "        \n",
    "        coeffs.update({'label': obj_id})\n",
    "        final_dict = props | coeffs\n",
    "        # logger.info(f\"Processed object {obj_id}\")\n",
    "        return final_dict\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing object {obj_id}: {e}\", exc_info=True)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-10 10:29:12,083 - INFO - Object 5796 too small (area=729.0). Skipping...\n",
      "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/aicsshparam/shparam.py:134: UserWarning: Mesh centroid seems to fall outside the object. This indicates        the mesh may not be a manifold suitable for spherical harmonics        parameterization.\n",
      "  warnings.warn(\n",
      "/net/beliveau/vol1/home/msforman/miniconda3/envs/otls-pipeline/lib/python3.13/site-packages/aicsshparam/shparam.py:134: UserWarning: Mesh centroid seems to fall outside the object. This indicates        the mesh may not be a manifold suitable for spherical harmonics        parameterization.\n",
      "  warnings.warn(\n",
      "2025-07-10 10:29:42,845 - INFO - Object 7767 too small (area=3862.0). Skipping...\n",
      "2025-07-10 10:29:45,136 - INFO - Object 3108 too small (area=73.0). Skipping...\n"
     ]
    }
   ],
   "source": [
    "lmax_times = []\n",
    "lmax_range = list(range(4, 32, 4))\n",
    "processed_objects = {}\n",
    "i = 1\n",
    "import time\n",
    "for lmax in lmax_range:\n",
    "    processed_objects[lmax] = []\n",
    "    for idx, obj in test_objects.iterrows():\n",
    "        # Extract bounding box\n",
    "        slice_z, slice_y, slice_x = obj[0], obj[1], obj[2]\n",
    "        bboxes = tuple([slice_z, slice_y, slice_x])    \n",
    "        # Crop mask and image to bounding box\n",
    "        mask_da_obj = mask_da[slice_z, slice_y, slice_x].compute()\n",
    "        image_da_obj = image_da[slice_z, slice_y, slice_x].compute()\n",
    "        # Process object\n",
    "        obj_series = pd.Series(obj, name=idx)  # Create series with object ID as name\n",
    "        result = process_object(obj_series, \n",
    "                                mask_da_obj, \n",
    "                                image_da_obj,\n",
    "                                lmax=lmax)\n",
    "        processed_objects[lmax].append(result)\n",
    "print(processed_objects)\n",
    "\n",
    "# def bootstrap_lmax_selection(self, processed_objects, n_bootstrap=100):\n",
    "#     lmax_errors = {}\n",
    "    \n",
    "#     for lmax in self.lmax_range:\n",
    "#         bootstrap_errors = []\n",
    "        \n",
    "#         for _ in range(n_bootstrap):\n",
    "#             # Sample objects with replacement\n",
    "#             sample = np.random.choice(processed_objects, \n",
    "#                                     size=len(processed_objects), \n",
    "#                                     replace=True)\n",
    "            \n",
    "#             sample_errors = []\n",
    "#             for obj in sample:\n",
    "#                 # Use existing optimized fitting\n",
    "#                 error = compute_reconstruction_error(obj, lmax)\n",
    "#                 sample_errors.append(error)\n",
    "            \n",
    "#             bootstrap_errors.append(np.mean(sample_errors))\n",
    "        \n",
    "#         lmax_errors[lmax] = np.mean(bootstrap_errors)\n",
    "    \n",
    "#     return min(lmax_errors.keys(), key=lambda k: lmax_errors[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otls-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
